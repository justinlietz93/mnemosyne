# ==============================================================================
# Project Prometheus: Aegis Safety Layer
# Version 0.1
#
# Agent: PrometheusAI
# Mission: To act as a real-time monitoring and intervention system, ensuring
#          the agent's responses adhere to defined safety and ethical rules.
#
# Description:
# This script defines the Aegis class, our foundational safety layer.
# Initially, it will perform basic content moderation. This module is designed
# to be expanded with more sophisticated rules and formal verification methods
# as the project evolves.
# ==============================================================================

class Aegis:
    """
    The Ethics & Safety Layer. It validates the kernel's output before it is
    sent to the user.
    """
    def __init__(self):
        print("Initializing Aegis Safety Layer...")
        # In a real-world scenario, this would be a sophisticated list of
        # harmful patterns, keywords, or classifiers. We start simple.
        self.forbidden_patterns = [
            "i can't answer that",
            "i am not programmed",
            # Add more patterns for harmful, unethical, or dangerous content here
        ]
        print("Aegis Safety Layer initialized.")

    def validate_response(self, response_text: str) -> bool:
        """
        Validates a response against the current safety rules.

        Args:
            response_text: The complete response generated by the LLM.

        Returns:
            True if the response is safe, False otherwise.
        """
        lower_response = response_text.lower()
        for pattern in self.forbidden_patterns:
            if pattern in lower_response:
                print(f"[Aegis] Validation FAILED. Response contains forbidden pattern: '{pattern}'")
                return False
        
        print("[Aegis] Validation PASSED.")
        return True

# --- Standalone Test ---
if __name__ == "__main__":
    print("Performing standalone test of Aegis layer...")
    aegis_system = Aegis()

    # Test Case 1: A safe response
    safe_response = "The sun is composed primarily of hydrogen and helium."
    print(f"\nTesting safe response: \"{safe_response}\"")
    is_safe = aegis_system.validate_response(safe_response)
    print(f"Is response safe? {is_safe}")
    assert is_safe is True

    # Test Case 2: A response that violates a rule
    unsafe_response = "I am not programmed to discuss that topic."
    print(f"\nTesting unsafe response: \"{unsafe_response}\"")
    is_safe = aegis_system.validate_response(unsafe_response)
    print(f"Is response safe? {is_safe}")
    assert is_safe is False
    
    print("\nAegis standalone tests complete.")

